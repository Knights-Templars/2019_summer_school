{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Classify AGN objects\n",
    "\n",
    "AGN classification , BPT , https://ned.ipac.caltech.edu/level5/Glossary/Essay_bpt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data from the MPA-JHU team's 'galSpec' table for the SDSS galaxies:\n",
    "\n",
    "PATH_AGN_data = \"data/sdss_lines.csv\"\n",
    "\n",
    "data = np.genfromtxt(PATH_AGN_data, delimiter=\",\")\n",
    "# The data file is organized in 452 lines (i.e. different objects), and 15 columns\n",
    "\n",
    "# To check file dimensions:\n",
    "# print(data.shape)\n",
    "\n",
    "ID             = data[:,0]  # object ID\n",
    "H_BETA_FLUX    = data[:,1]  # flux of H_beta line\n",
    "OIII_5007_FLUX = data[:,3]  # flux of OIII line at 5007 Amstrong\n",
    "H_ALPHA_FLUX   = data[:,7]  # flux of H_alpha line\n",
    "NII_6584_FLUX  = data[:,9]  # flux of NII line at 6584 Amstrong\n",
    "SII_6717_FLUX  = data[:,12] # flux of SII line at 6717 Amstrong\n",
    "\n",
    "labels = np.genfromtxt(PATH_AGN_data, delimiter=',', usecols=-2, dtype=str)\n",
    "# reading labels from second column from the last\n",
    "\n",
    "# > Replacing negative (bogus) fluxes with sample median:\n",
    "#   NOTE: This will introduce some noise in the data, which is good for the sake of this exercise\n",
    "OIII_5007_FLUX[OIII_5007_FLUX <= 0] = np.median(OIII_5007_FLUX)\n",
    "NII_6584_FLUX[NII_6584_FLUX   <= 0] = np.median(NII_6584_FLUX)\n",
    "SII_6717_FLUX[SII_6717_FLUX   <= 0] = np.median(SII_6717_FLUX)\n",
    "H_BETA_FLUX[H_BETA_FLUX       <= 0] = np.median(H_BETA_FLUX)\n",
    "H_ALPHA_FLUX[H_ALPHA_FLUX     <= 0] = np.median(H_ALPHA_FLUX)\n",
    "\n",
    "# > Calculating diagnostic ratios:\n",
    "OIII_diagnostic = np.log10(OIII_5007_FLUX / H_BETA_FLUX)  # OIII_5007 / H_beta\n",
    "NII_diagnostic  = np.log10(NII_6584_FLUX  / H_ALPHA_FLUX) # NII_6584  / H_alpha\n",
    "SII_diagnostic  = np.log10(SII_6717_FLUX  / H_ALPHA_FLUX) # SII_6717  / H_alpha\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X = np.stack((OIII_diagnostic,NII_diagnostic,SII_diagnostic),axis=-1)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Summary\n",
      "________________________________\n",
      "Total objects    | 452  \n",
      "-----------------|--------------\n",
      " '-> Training    | 361  \n",
      "                 | (361, 3)\n",
      "      '-> AGN    | 34    (9.4%)\n",
      "      '-> no AGN | 327   (90.6%)\n",
      "-----------------|--------------\n",
      " '-> Validation  | 45   \n",
      "                 | (45, 3)\n",
      "      '-> AGN    | 3     (6.7%)\n",
      "      '-> no AGN | 42    (93.3%)\n",
      "-----------------|--------------\n",
      " '-> Test        | 46   \n",
      "                 | (46, 3)\n",
      "      '-> AGN    | 4     (8.7%)\n",
      "      '-> no AGN | 42    (91.3%)\n"
     ]
    }
   ],
   "source": [
    "# SPLITTING SAMPLE IN TRAINING, VALIDATION, AND TEST\n",
    "\n",
    "# Fractions of total sample to be re-distributed into training, validation, and test\n",
    "train_frac = 0.80\n",
    "valid_frac = 0.10\n",
    "test_frac  = 0.10\n",
    "\n",
    "(X_train, X_remain), (y_train, y_remain) = split_samples(X, y, [train_frac, (valid_frac+test_frac)], random_state=0)\n",
    "# isolating first the training set ...\n",
    "\n",
    "(X_valid, X_test), (y_valid, y_test)  = split_samples(X_remain, y_remain, [test_frac, valid_frac], random_state=0)\n",
    "# ... and now repeting to split validation and training sets\n",
    "\n",
    "n_train_AGN   = len([y for y in y_train if y == 'true'])\n",
    "n_train_noAGN = len([y for y in y_train if y == 'false'])\n",
    "\n",
    "n_valid_AGN   = len([y for y in y_valid if y == 'true'])\n",
    "n_valid_noAGN = len([y for y in y_valid if y == 'false'])\n",
    "\n",
    "n_test_AGN    = len([y for y in y_test if y == 'true'])\n",
    "n_test_noAGN  = len([y for y in y_test if y == 'false'])\n",
    "\n",
    "\n",
    "print(\"Sample Summary\")\n",
    "print(\"________________________________\")\n",
    "print(\"Total objects    | %-5s\" % len(X))\n",
    "print(\"-----------------|--------------\")\n",
    "print(\" '-> Training    | %-5s\" % len(X_train))\n",
    "print(\"                 | \" + str(X_train.shape))\n",
    "print(\"      '-> AGN    | %-5s (%-.1f%%)\" % (n_train_AGN,   (n_train_AGN/len(X_train)*100.)))\n",
    "print(\"      '-> no AGN | %-5s (%-.1f%%)\" % (n_train_noAGN, (n_train_noAGN/len(X_train)*100.)))\n",
    "print(\"-----------------|--------------\")\n",
    "print(\" '-> Validation  | %-5s\" % len(X_valid))\n",
    "print(\"                 | \" + str(X_valid.shape))\n",
    "print(\"      '-> AGN    | %-5s (%.1f%%)\" % (n_valid_AGN,   (n_valid_AGN/len(X_valid)*100.)))\n",
    "print(\"      '-> no AGN | %-5s (%.1f%%)\" % (n_valid_noAGN, (n_valid_noAGN/len(X_valid)*100.)))\n",
    "print(\"-----------------|--------------\")\n",
    "print(\" '-> Test        | %-5s\" % len(X_test))\n",
    "print(\"                 | \" + str(X_test.shape))\n",
    "print(\"      '-> AGN    | %-5s (%.1f%%)\" % (n_test_AGN,   (n_test_AGN/len(X_test)*100.)))\n",
    "print(\"      '-> no AGN | %-5s (%.1f%%)\" % (n_test_noAGN, (n_test_noAGN/len(X_test)*100.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Is ithe sample shaped as expected?\n",
    "It should be split as < N x M >, where N is the number of objects (with relative fractions given by the train, validation, and test fractions) and M the number of features we can use for the classification.\n",
    "\n",
    "#### Q: Can we really use the validation/test sample?\n",
    "No, the statistics are too low. That means for example that we cannot calibrate hyperparameters such as the $k$ in the KNN via cross-validation.\n",
    "\n",
    "**NOTE:** Addditionally, keep in mind the \"demographics\" for the training and validation samples (i.e. how many AGNs), and later compare them with the accuracy of each classifier.\n",
    "If accuracies are similar to the demographics, the classifier is only mirroring the data (i.e. overfitting)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "model_svc = LinearSVC()\n",
    "#model_svc = NuSVC()\n",
    "model_svc.fit(data_train[:,:,5], labels_train)\n",
    "# NOTE: using only 1 band\n",
    "\n",
    "# Comparisong with prediction\n",
    "predicted = model_svc.predict(data_valid[:,:,5])\n",
    "\n",
    "print(\"Classification report for %s:\\n%s\\n\"\n",
    "      % (model_svc, metrics.classification_report(labels_valid, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(labels_valid, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading the necessary packages and data\n",
    "\n",
    "The data for the example will be retrieved via the `astroML` package (see `astroML.datasets`). From this, we will fetch a sample of SDSS (Sloan Digital Sky Survey) stars including both RR Lyrae and any other type of stars. The `fetch_rrlyrae_combined` function (below) downloads two arrays:\n",
    "* a $N \\times 4$ array for the four colors $g-r$, $u-g$, $r-i$, $i-z$ of $N \\approx 100K$ stars\n",
    "* a $N$-sized array of zeros and ones, where \"$1$\" is the label which denotes that the respective star is an RR Lyrae\n",
    "\n",
    "### A technical workaround\n",
    "\n",
    "As the data to download is about $70$ MB (and may take a while with the public wifi network) and is permenently stored in your computer, we downloaded the data and stored it in a compressed `numpy` format. The function that performs exactly that, is `download_and_save_RRlyrae_data()` and we include it only for future reference.\n",
    "\n",
    "Now, the script only requires calling `load_data()` and having the file `RRlyrae.npz` in the working directory. In case the latter is missing, just uncomment the call to the function `download_and_save_RRlyrae_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "\n",
    "PATH = \"RRlyrae.npz\"\n",
    "\n",
    "\n",
    "def download_and_save_RRlyrae_data():\n",
    "    \"\"\"Create the local file containing photometry and classification of stars in SDSS sample.\"\"\"\n",
    "    \n",
    "    # get data and split into training & testing sets\n",
    "    print(\"Getting RR Lyrae data...\")\n",
    "    X, y = fetch_rrlyrae_combined()\n",
    "    X = X[:, [1, 0, 2, 3]]           # rearrange columns for better plots\n",
    "    np.savez_compressed(PATH, colors=X, isRR=y)\n",
    "    print(\"    Done.\")\n",
    "    \n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"Load RR Lyrae data from local file.\"\"\"\n",
    "    \n",
    "    print(\"Loading data from {}...\".format(PATH))\n",
    "    data = np.load(PATH)\n",
    "    colors, isRR = data[\"colors\"], data[\"isRR\"]\n",
    "    print(\"    Done.\")\n",
    "    print(\"    {:6} objects in total\".format(len(isRR)))\n",
    "    print(\"    {:6} RR Lyrae in data\".format(int(sum(isRR))))\n",
    "    return colors, isRR\n",
    "\n",
    "\n",
    "# if the file is missing, uncomment the following line\n",
    "# download_and_save_RRlyrae_data()\n",
    "\n",
    "# load photometry and classification (labels) of training data\n",
    "X, y = load_data()\n",
    "# X = <Nx4 colors>\n",
    "# y = <N> labels\n",
    "\n",
    "# Create boolean \"mask\" arrays denoting classification as RR Lyrae:\n",
    "# (will be useful from data exploration/plotting)\n",
    "isRR = (y == 1)\n",
    "noRR = (y == 0)\n",
    "\n",
    "# Limit scatter plots (not histograms) in showing a maximum of 5000 non-RR Lyrae stars:\n",
    "N_plot = 5000 + int(sum(y))\n",
    "noRR[:-N_plot] = False\n",
    "\n",
    "# names of the colors\n",
    "color_names = [\"$g-r$\", \"$u-g$\", \"$r-i$\", \"$i-z$\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring the data\n",
    "\n",
    "### Color distributions\n",
    "\n",
    "We will start easy by creating color histograms for \"standard\" and RR Lyrae stars to see if they are separable by using just one colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    color = X[:, i]\n",
    "    bins = np.linspace(np.nanmin(color), np.nanmax(color), 31)\n",
    "    \n",
    "    plt.subplot(221 + i)\n",
    "    plt.hist(color[isRR],  bins=bins, log=True, color=\"r\", histtype=\"step\", label=\"RR lyrae\")\n",
    "    plt.hist(color[~isRR], bins=bins, log=True, color=\"k\", histtype=\"step\", label=\"stars\")\n",
    "    plt.xlabel(color_names[i]  , fontsize=14)\n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer 2D look\n",
    "Maybe using color-color diagrams we can easily classify RR Lyrae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "k = 1\n",
    "for i in range(4):\n",
    "    c1 = X[:, i]\n",
    "    for j in range(i + 1, 4):\n",
    "        c2 = X[:, j]\n",
    "        plt.subplot(320 + k)\n",
    "        plt.plot(c1[noRR], c2[noRR], \"k.\", label=\"stars\")\n",
    "        plt.plot(c1[isRR], c2[isRR], \"r.\", label=\"RR lyrae\")\n",
    "        plt.xlabel(color_names[i], fontsize=14)\n",
    "        plt.ylabel(color_names[j], fontsize=14)\n",
    "        plt.legend(loc=\"upper right\", framealpha=0.9, mode=\"expand\", ncol=2, fontsize=14)\n",
    "        k += 1\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As close as we can *see* (3D)\n",
    "What if we plot three colors? Of course we cannot continue with four colors because of our 3D perception limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "combinations = [(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "for index, combination in enumerate(combinations):\n",
    "    i, j, k = combination\n",
    "    ax = fig.add_subplot(221 + index, projection='3d')\n",
    "    ax.view_init(60, -130)  # set camera position for better visualization\n",
    "    ax.scatter(X[:, i][noRR], X[:, j][noRR], X[:, k][noRR], c=[0.5,0.7,0.7], marker=\"o\", alpha=0.5, edgecolors=\"k\", label=\"stars\")\n",
    "    ax.scatter(X[:, i][isRR], X[:, j][isRR], X[:, k][isRR], c=\"r\"                                 , edgecolors=\"k\", label=\"RR Lyrae\")\n",
    "    ax.set_xlabel(color_names[i], fontsize=14)\n",
    "    ax.set_ylabel(color_names[j], fontsize=14)\n",
    "    ax.set_zlabel(color_names[k], fontsize=14)\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D plots do not significantly improve the *manual classification*, compared to the 2D plots. In both cases, we see a *tail* where RR Lyrae variables reside.\n",
    "\n",
    "### Question\n",
    "What we would be compromizing if we were to separate RR Lyrae by applying simple horizontal/vertical \"cuts\" in the 1D, 2D, or 3D color distributions?\n",
    "\n",
    "### The need for a ML classifier\n",
    "We can improve over the limitations of the *manual classification* by obtaining:\n",
    "* mathematical formulation\n",
    "* automation\n",
    "* estimates on completeness and contamination of the classification method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $k$-Nearest Neighbors (KNN) classification\n",
    "\n",
    "Looking at the scatter plots, we find two blobs corresponding to the different classes. Our mind uses the proximity of the points to form a \"mental boundary region\" (in a _discriminative classification_ fashion). In this specific case we can try to automate of the procedure, as well as the extension to arbitrary number of dimensions, by mimicking the human behaviour, i.e. by:\n",
    "\n",
    "> attributing a given point to the class that dominates its surroundings.\n",
    "\n",
    "The problem is then how to define the \"neighborhood\" of a point. The trivial solution would be to set a fixed radius. The issue then becomes its size: if too small, we **will not find neighbors** for \"satellite\" points at the edge of a class cluster; if too large, we **will lose resolution** in dense parts, effectively throwing away information. Therefore, ideally we would like to have a *variable bandwidth* selection threshold.\n",
    "\n",
    "> One solution is to use a local average of the labels of the $k$ nearest neighbors:\n",
    ">\n",
    "> $y = \\frac{1}{k} \\sum￼_{x_i \\in N_k(x)} y_{i}$\n",
    ">\n",
    "> where N_k(x) is the neighborhood around $x_i$\n",
    "\n",
    "In this way the classification _is not_ defined based on the distance on the parameter graph, but is rather scale-independent.\n",
    "\n",
    "Let's see a 2D example. We got two parameters and training data that are classified as being *red* or *blue*. The question is how do we classify a new (_i.e. not part of the training set_) point? The following images are taken from [3] (we edited the the first one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated example\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_initial.jpg\">\n",
    "        <center>Figure 3.1.a. Training data already possessing a _red_ or _blue_ label, and an arbitrary new point to be classified.</center>\n",
    "    </td>    \n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh.gif\">\n",
    "        <center>Figure 3.1.b. Classification using majority votes of $k$ neighbors, fordifferent values of $k$.</center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_mult.gif\">\n",
    "        <center>Figure 3.1.c. For a fixed $k$, the **model** can be thought as of a function of the location in the parameter space. Note that the appearing dots are _not_ part of the training set. Instead, they represent the predicted classifications **if** the new point would fall on that position.</center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The panels *a* and *b* already suggest that the KNN classification will be affected by the choice of the **hyperparameter** $k$: we will address this issue later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Applying $k$-NN on the RR Lyrae photometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample in a training [75% of total] and test [25% of total] subsets:\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)          # number of stars\n",
    "N_st = np.sum(y == 0)   # number of non-RR Lyrae stars\n",
    "N_rr = N_tot - N_st     # number of RR Lyrae\n",
    "N_train = len(y_train)  # size of training sample\n",
    "N_test = len(y_test)    # size of test sample\n",
    "N_plot = 5000 + N_rr    # number of stars plotted (for better visualization)\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)    # number of available colors\n",
    "\n",
    "print(\"Sample size\")\n",
    "print(\"----------------\")\n",
    "print(\"Total           | %d\" % (len(X)))\n",
    "print(\" '-> Train      | %d\" % (len(X_train)))\n",
    "print(\"     data shape |\", ((X_train.shape)))\n",
    "print(\" '-> Test       | %d\" % (len(X_test)))\n",
    "print(\"     data shape |\", ((X_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLASSIFICATION FOR VARIOUS VALUES OF k\n",
    "\n",
    "# for each 'k', store the classifier and predictions on test sample\n",
    "classifiers = []\n",
    "predictions = []\n",
    "kvals = [1, 3, 10] # k values to be used\n",
    "\n",
    "for k in kvals:\n",
    "# we try different k hyperparameters\n",
    "\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    \n",
    "    for nc in Ncolors:\n",
    "    # for each k, we will use between 1 and 4 colors (1D to 4D) to classify the sources\n",
    "    \n",
    "        clf = KNeighborsClassifier(n_neighbors=k) # define the classifier, in this case KNN with hyperparameter k\n",
    "        clf.fit(X_train[:, :nc], y_train)         # fit training data\n",
    "        y_pred = clf.predict(X_test[:, :nc])      # predict class of test data\n",
    "\n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "# Use dedicated astroML to obtain completeness and contamination:\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"--------\")\n",
    "\n",
    "for i,k in enumerate(kvals):\n",
    "    print(\"k =\", k)\n",
    "    for nc in Ncolors:\n",
    "        print(\"\\tno. of colors = %s | \" % (nc), end=\"\")\n",
    "        print(\"completeness: %.2f \" % (completeness[i,nc-1]), end=\"\")\n",
    "        print(\"contamination: %.2f\" % (contamination[i,nc-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND PLOT DECISION BOUNDARY\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# classifiers\n",
    "# <N_k, N_colors>\n",
    "\n",
    "clf = classifiers[1][1]\n",
    "# classifier with k=3 and the first 2 colors (g-r, and u-g)\n",
    "xlim = (-0.15, 0.4) # g-r limits\n",
    "ylim = (0.7, 1.35)  # u-g limits\n",
    "\n",
    "# Creating a grid Z of predictions:\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71), np.linspace(ylim[0], ylim[1], 81))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Creating a colormap where the RR lyrae (y label = \"1\") are plotted in red\n",
    "cmap = mpl.colors.ListedColormap(['black', 'red'])\n",
    "cmap.set_under('0.5')\n",
    "cmap.set_over('0.5')\n",
    "\n",
    "# PLOT THE RESULTS\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# > left plot: data and decision boundary\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 0], X[-N_plot:, 1], c=y[-N_plot:], s=10, lw=0, cmap=cmap, zorder=2)\n",
    "im.set_clim(-0.5, 1.5)\n",
    "ax.contour(xx, yy, Z, 1, colors='blue')\n",
    "# NOTE: The contour showing the locus where a datum is classified as \"RR Lyrae\" is simply\n",
    "#       calculated on the grid of predictions\n",
    "#       To visualize the grid of predictions instead of the contour, use:\n",
    "#im = ax.imshow(Z, origin='lower', aspect='auto', cmap=cmap, zorder=1, extent=xlim + ylim)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('Decision boundary for classification', fontsize=14)\n",
    "ax.set_xlabel('$g-r$', fontsize=14)\n",
    "ax.set_ylabel('$u-g$', fontsize=14)\n",
    "ax.text(0.02, 0.02, \"k = %i\" % kvals[1], transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > top-right plot: Completeness vs Ncolors\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], 'o-k' , ms=6, label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label='k=%i' % kvals[1])\n",
    "ax.plot(Ncolors, completeness[2], 'v:k' , ms=6, label='k=%i' % kvals[2])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_title('Classifier performance', fontsize=14)\n",
    "ax.set_ylabel('completeness', fontsize=14)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# > bottom-right plot: contamination vs Ncolors\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k' , label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', label='k=%i' % kvals[1])\n",
    "ax.plot(Ncolors, contamination[2], 'v:k' , label='k=%i' % kvals[2])\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.0, 0.79), fontsize=14)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('N colors', fontsize=14)\n",
    "ax.set_ylabel('contamination', fontsize=14)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the coarsness of the decision boundary (blue contour), which indicates a potential over-fitting of the training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Choosing the right $k$\n",
    "\n",
    "### Small $k$ \n",
    "> $\\large+$ only nearby points are taken into account\n",
    "\n",
    "> $\\large-$ if too small, noisy decision boundaries (see change of classification in Figure 3.1.b for small $k$'s)\n",
    "\n",
    "### Large $k$\n",
    "> $\\large+$ assuming infinite sample for $k \\rightarrow \\infty$, the *classification error rate* is minimized\n",
    "\n",
    "> $\\large-$ **but** real-life samples are finite, so large $k$ results to loss of resolution (over-smoothing)\n",
    "\n",
    "### Some approaches\n",
    "\n",
    "> take $k = \\sqrt{N}$\n",
    "\n",
    "> use *cross-validation* to select optimal $k$\n",
    "\n",
    "> if $2$ classes, go for an odd $k$ to avoid ties\n",
    "\n",
    "Figure 3.2 (from [4]) reports an example of over-smoothing.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/k1.jpg\"   width=400></td>    \n",
    "        <td><img src=\"images/k5.jpg\"   width=400></td>\n",
    "        <td><img src=\"images/k399.jpg\" width=400></td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <center>\n",
    "            Figure 3.2. Variation of classification boundaries as $k$ increases to very large values.\n",
    "        </center>    \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 3.3. Defining the neighborhood\n",
    "What does \"near\" in \"nearest neighbors\" mean?\n",
    "Identifying neighbors requires calculating a **distance** between points or, better said, define a **metric**. A metric might be difficult to define or might not be definable at all for the type of features we are inspecting (e.g. imagine classifying galaxies by morphology and color). Consider for example the following issues.\n",
    "\n",
    "* One simple choice would be to adopt the common Euclidean metric. However that would not be performing properly when the parameter spans are different (e.g. one parameter has span $\\left[1, 100\\right]$ while the other has span $\\left[0, 1\\right]$). This might require **feature weighting** (normalization).\n",
    "* What if all or some characteristics are not numeric but categorial? What is the metric in this case (see Hamming distance)?\n",
    "\n",
    "For more information, consult [5].\n",
    "\n",
    "### 3.4. Weighted KNN ($w$KNN)\n",
    "We can mitigate the impact of choosing the appropriate $k$ by imposing that each vote on the class is weighted by the distance (or *similarity*) from the point to be classified. In this case, the classification algorithm for point $x$ becomes:\n",
    "\n",
    "> $y = \\frac{1}{k} \\sum￼_{x_i \\in N_k(x)} ~K(d) \\times y_{i}$\n",
    ">\n",
    "> where $d$ is the distance and $K(d)$ is the **kernel** which converts the distance into a weight. One simple example is the inversion kernel $K(d) = \\frac{1}{d}$\n",
    "\n",
    "For an extensive treatment, consult [6].\n",
    "\n",
    "### 3.5. Final remarks on KNN\n",
    "\n",
    "### Pros\n",
    "* No need to assume distribution (_discriminative classifier_)\n",
    "* Simple and intuitive\n",
    "* Robust for large samples\n",
    "\n",
    "### Cons\n",
    "* Hard to select $k$\n",
    "* Computationally expensive: $O(nkd)$, where $n$ is the size of training sets, and $d$ is the dimension of each training set (but can be optimised)\n",
    "* Good accuracy requires large samples unifromly covering the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machine (SVM) is a way of choosing a linear decision boundary between different classes.\n",
    "\n",
    "The classification boundary is provided by the hyperplane maximizing the distance between the hyperplane itself and the closest point from either class. This distance is called *margin*. Points on the margins are called *support vectors*.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_1.png\">\n",
    "        <center>Figure 4.1.a. Hyperplane (dashed line) separating two classes (_red_ and _green_).</center>\n",
    "    </td>    \n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_2.png\">\n",
    "        <center>Figure 4.1.b. The closest points to the hyperplane from each class constitute the \"tip\" of the *support vectors*.</center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Figure 4.1.a shows two different classes (e.g. young and old stars) distributing in a scatter plot according to variable $x_1$ (e.g. radius) and $x_2$ (e.g. temperature). Figure 4.1.b explains the origin of the name \"support vectors\": the closest points _support_ the hyperplanes (solid lines) equally distant from the decision hyperplane (dashed line).\n",
    "\n",
    "Infinite possible boundaries can separate the two classes. SVM algorithms find the one that maximizes the distance between the supported hyperplanes.\n",
    "\n",
    "### 4.1. Hyperplanes and decision boundary\n",
    "\n",
    "The supported hyperplanes (solid-lines in Figure 4.1.a can be defined as:\n",
    "\n",
    "> **w**$\\cdot$**x** + b = +1\n",
    ">\n",
    "> **w**$\\cdot$**x** + b = -1\n",
    "\n",
    "where **x** is the coordinate on the (x1, x2) plane, **w** is a 2$\\times$1 matrix and **b** a scalar. It turns out that these hyperplanes are separated by a distance 2 / ||w||. Finding the ideal classification boundary, i.e. the one maximizing the distance, is therefore a problem of minimizing the norm ||w||. This is what SVM algorithms do.\n",
    "\n",
    "For an complete mathematical formulation, consult [7].\n",
    "\n",
    "### 4.2. Separatable classes (or not)\n",
    "\n",
    "We cannot always assume that 2 classes are separble without \"contamination\". That is why SVM algorithms include **slack variables**, a tunable parameter which penalizes misclassifications.\n",
    "\n",
    "\n",
    "### 4.3. Multiple classes\n",
    "\n",
    "The SVM method can be applied for multiple classes as well (Figure 4.3).\n",
    "\n",
    "<img src=\"images/svm_many_classes.png\" width=400>\n",
    "<center>\n",
    "    Figure 4.3: SVM applied to 3 different classes.\n",
    "</center>\n",
    "\n",
    "### 4.4. Multiple dimensions\n",
    "\n",
    "If our sample of stars is characterized by three parameters (X, Y, Z), e.g. radius, temperature and mass, then the scatter plot has 3 dimensions (Figure 4.3). The boundary between the classes in the 3-D plot is a plane. Because of the fact that the method can be extrapolated at N-dimensions, the boundary is a *hyperplane*.\n",
    "\n",
    "<img src=\"images/svm_3d.png\" width=400>\n",
    "<center>\n",
    "    Figure 4.4: Support vector machine applied for 3-D features and three classes.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Applying linear SVM to the RR Lyrae photometric data\n",
    "\n",
    "We will use SVM to our RR Lyrae example to separate objects with a linear decision boundary.\n",
    "In $\\S$4.6 and $\\S$4.7 we will see an extension of SVM adopting non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block, we define a function that applies SVM on our data.\n",
    "# If the boolean argument is set to `True`, it applies the linear SVM (this section)\n",
    "# otherwise it uses Gaussian Radial Basis function kernel (RBF; Section 4.7).\n",
    "\n",
    "def apply_SVM(linear):\n",
    "    if linear:\n",
    "        kernel_to_use = \"linear\"\n",
    "        gamma_to_use = \"auto\"\n",
    "    else:\n",
    "        kernel_to_use = \"rbf\"\n",
    "        gamma_to_use = 20.0\n",
    "\n",
    "    def compute_SVM(Ncolors):\n",
    "        classifiers = []\n",
    "        predictions = []\n",
    "        \n",
    "        for nc in Ncolors:\n",
    "            print(\"    Computing for\", nc, \"color(s)...\")\n",
    "            # perform support vector classification\n",
    "            clf = SVC(kernel=kernel_to_use, gamma=gamma_to_use, class_weight='balanced')\n",
    "            clf.fit(X_train[:, :nc], y_train)\n",
    "            y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "            classifiers.append(clf)\n",
    "            predictions.append(y_pred)\n",
    "\n",
    "        return classifiers, predictions\n",
    "\n",
    "    print(\"Performing SVM classification...\")\n",
    "\n",
    "    classifiers, predictions = compute_SVM(Ncolors)\n",
    "\n",
    "    completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "    print(\"completeness\",  completeness)\n",
    "    print(\"contamination\", contamination)\n",
    "\n",
    "    # COMPUTE THE DECISION BOUNDARY\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: The sklearn SVM routine does _not_ return the coefficients of the\n",
    "          decision boundary.\n",
    "          For example, for a 2D case where the decision boundary is a line\n",
    "          of the type:\n",
    "    \n",
    "             y = a*x + b\n",
    "    \n",
    "          one would expect to obtain \"a\" and \"b\" (where b == intercept).\n",
    "          Instead, the w matrix is returned (see Section 4.1).\n",
    "          In the 2D case, for a linear model, the conversion is trivial\n",
    "          (see the code below for a linear case).\n",
    "          \n",
    "          For a more generic case of representing a curved boundary in N\n",
    "          dimensions, it is more convenient to generate a grid (\"Z\") of\n",
    "          predictions covering the whole parameter space (essentially a map\n",
    "          of the classes), and then plot the contour around a class\n",
    "          (see the code below for a non-linear case).\n",
    "          \n",
    "          Alternatively, one can use the sklearn built-in function to\n",
    "          draw the decision function:\n",
    "\n",
    "              https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = classifiers[1]\n",
    "    # loading classifier for 2 colors\n",
    "\n",
    "    if linear:\n",
    "        # > Extracting slope and intercept of boundary line:\n",
    "        w = clf.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        yy = np.linspace(-0.1, 0.4)\n",
    "        xx = a * yy - clf.intercept_[0] / w[1]\n",
    "    else:\n",
    "        # > Creating a grid of predictions:\n",
    "        xlim = (0.7, 1.35)\n",
    "        ylim = (-0.15, 0.4)\n",
    "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101), np.linspace(ylim[0], ylim[1], 101))\n",
    "        Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        # Smooth the boundary:\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        Z = gaussian_filter(Z, 2)\n",
    "\n",
    "    # PLOT THE RESULTS\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "    # left plot: data and decision boundary\n",
    "    ax = fig.add_subplot(121)\n",
    "    im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:], s=4, lw=0, cmap=plt.cm.binary, zorder=2)\n",
    "    if linear:\n",
    "        ax.plot(xx, yy, '-k')\n",
    "    else:\n",
    "        ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "    \n",
    "    im.set_clim(-0.5, 1)\n",
    "    ax.set_xlim(0.7, 1.35)\n",
    "    ax.set_ylim(-0.15, 0.4)\n",
    "    ax.set_xlabel('$u-g$', fontsize=14)\n",
    "    ax.set_ylabel('$g-r$', fontsize=14)\n",
    "\n",
    "    # plot completeness vs Ncolors\n",
    "    ax = fig.add_subplot(222)\n",
    "    ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_ylabel('completeness', fontsize=14)\n",
    "    ax.set_xlim(0.7, 4.5)\n",
    "    ax.set_ylim(0.7, 1.1)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # plot contamination vs Ncolors\n",
    "    ax = fig.add_subplot(224)\n",
    "    ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "    ax.set_xlabel('N colors',      fontsize=14)\n",
    "    ax.set_ylabel('contamination', fontsize=14)\n",
    "    ax.set_xlim(0.7, 4.5)\n",
    "    ax.set_ylim(0.7, 1.1)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_SVM(linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Non-linear boundaries\n",
    "\n",
    "Sometimes, linear boundaries may not be optimal and a non-linear SVM should be used instead. The left panel of Figure 4.6 shows an 2D scatter plot of two different classes (e.g. red and green stars with different radii and temperatures) which cannot be linearly separated.\n",
    "\n",
    "In order to find non-linear boundaries we can tackle the problem in an higher dimensional space. We use a process called **kernelization**, which consists in using a kernel function to attribute to our data a value in the additional dimension. Then, we draw the decision hyperplane into this higher dimensional space.\n",
    "\n",
    "The central panel of Figure 4.6 shows that once the 2D data are mapped to a 3D space by attributing a $z$ value through a Gaussian-like function, the classes are easily separable by a 3D hyperplane. Projecting back the plane in 2D, we obtain the non-linear boundary (Figure 4.6, rght panel).\n",
    "\n",
    "<img src=\"images/kernel.png\" width=800>\n",
    "<center>\n",
    "    Figure 4.6. When no linear boundaries can be used the SVM method can be applied by using kernel.\n",
    "</center>\n",
    "\n",
    "### Choosing the kernel function\n",
    "\n",
    "Useful kernel functions shall satisfy specific conditions (see [7]), so that in practice only a few are used. In the example of Figure 4.6, the Gaussian Radial Basis Function is used:\n",
    "\n",
    "> $K(x,y) = e^{-\\gamma(x-y)^2}$\n",
    "\n",
    "where $\\gamma$ is a hyperparameter which shall be learned via cross-validation (in our example we use an arbitrary value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Applying kernelized SVM to the RR Lyrae photometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_SVM(linear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Misclassifications\n",
    "\n",
    "Figure 4.3 shows that the blue points class is contaminated by some green \n",
    "points (*misclassified* points). Likewise, the green points class is contaminated by blue points. This contamination makes it difficult to define the boundary between the two classes.\n",
    "\n",
    "In this situation SVM finds the hyperplane that maximizes the margin and indirectly minimizes the misclassifications. However SVM is not designed to minimize the contamination _per se_.\n",
    "\n",
    "\n",
    "### 4.9. Final remarks on SVM\n",
    "\n",
    "### Pros\n",
    "* Good at dealing with high dimensional data\n",
    "* Works well on small data sets\n",
    "\n",
    "### Cons\n",
    "* Picking the right kernel and parameters can be computationally intensive\n",
    "* It suffers from contamination\n",
    "\n",
    "\n",
    "For further information on SVM, see [8]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] http://esoads.eso.org/abs/2003LNP...635...85B\n",
    "\n",
    "[2] https://www.eso.org/sci/publications/messenger/archive/no.13-jun78/messenger-no13-15-17.pdf\n",
    "\n",
    "[3] https://importq.wordpress.com/2017/11/24/mnist-analysis-using-knn/\n",
    "\n",
    "[4] https://idc9.github.io/stor390/notes/cross_validation/cross_validation.html\n",
    "\n",
    "[5] http://www.cs.haifa.ac.il/~rita/ml_course/lectures/KNN.pdf\n",
    "\n",
    "[6] https://epub.ub.uni-muenchen.de/1769/1/paper_399.pdf\n",
    "\n",
    "[7] http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\n",
    "\n",
    "[8] http://www.saedsayad.com/support_vector_machine.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
