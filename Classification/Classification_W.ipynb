{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set for the current Workshop\n",
    "\n",
    "Emission line ratios strongly depend on the heating mechanism which generates them: they can therefore be used to classify sources.\n",
    "In particular, in literature it is common to find a ($x$,$y$) plot with 2 emission line ratios (hereafter **diagnostic**), and one or more theoretical (or empirical) **curves** separating different objects.\n",
    "\n",
    "This technique is widespread, and it is routinely adopted for the classification of both galactic and extragalactic sources (see also the \"Clustering\" session).\n",
    "\n",
    "In this exercise we will use **machine learning** classification - as opposed to a simple curve - to separate object types.\n",
    "Additionally, we will expend the analysis to higher dimensions, i.e. beyond the canonical ($x$,$y$) plane which exploits only 2 diagnostics at a time.\n",
    "\n",
    "In particular, we will use a set of 7 diagnostics generated by simulations reproducing the emission of \"starforming\" and \"shock-heated\" objects.\n",
    "\n",
    "### Loading and setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data and classifications:\n",
    "\n",
    "PATH_lines_data = \"data/emission_lines.txt\" \n",
    "\n",
    "data = np.genfromtxt(PATH_lines_data)\n",
    "# The data file is organized in 8080 lines (i.e. different objects), and 10 columns\n",
    "\n",
    "diagnostic_1 = data[:,3]\n",
    "diagnostic_2 = data[:,4]\n",
    "diagnostic_3 = data[:,5]\n",
    "diagnostic_4 = data[:,6]\n",
    "diagnostic_5 = data[:,7]\n",
    "diagnostic_6 = data[:,8]\n",
    "diagnostic_0 = data[:,9]\n",
    "\n",
    "labels = np.genfromtxt(PATH_lines_data, delimiter=' ', usecols=0, dtype=str)\n",
    "# reading labels from first column\n",
    "# Emission class labelling scheme:\n",
    "#   0 <-> starburst\n",
    "#   1 <-> shocked\n",
    "\n",
    "# Dictionary containg class name and associated label:\n",
    "from collections import OrderedDict\n",
    "classes = OrderedDict()\n",
    "classes[\"starburst\"] = 0\n",
    "classes[\"shocked\"]   = 1\n",
    "\n",
    "labels = [int(float(label)) for label in labels]\n",
    "# converting labels from strings to integers\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "X_sample = np.stack((diagnostic_0,\n",
    "              diagnostic_1,\n",
    "              diagnostic_2,\n",
    "              diagnostic_3,\n",
    "              diagnostic_4,\n",
    "              diagnostic_5,\n",
    "              diagnostic_6),axis=-1)\n",
    "y_sample = labels\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"_____________________________________\")\n",
    "print('  X  | ' + str(X_sample.shape))\n",
    "print('     | ' + str(X_sample.shape[0]) + ' sammples x ' + str(X_sample.shape[1]) + ' diagnostics' )\n",
    "print(\"-----|-------------------------------\")\n",
    "print('  y  | ' + str(len(y_sample)) + ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   blue <-> starburst\n",
    "#   red  <-> shocked\n",
    "cmap = mpl.colors.ListedColormap(['blue','red'])\n",
    "\n",
    "# Remember that the sample X is organized as:\n",
    "#  X[:,0] <-> diagnostic 0\n",
    "#  X[:,1] <-> diagnostic 1\n",
    "#  X[:,2] <-> ...\n",
    "\n",
    "\n",
    "# PLOT EACH DIAGNOSITCS AGAINST THE OTHERS\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.4, left=0.1, right=0.95, wspace=0.5)\n",
    "\n",
    "plot_counter= 0\n",
    "n_rows = int(X_sample.shape[1]) - 1\n",
    "n_cols = int(X_sample.shape[1]) - 1\n",
    "\n",
    "for j in range(n_cols):\n",
    "    for i in range(n_rows):\n",
    "\n",
    "\n",
    "            plot_counter+=1\n",
    "            xlim = [np.min(X_sample[:, i]),np.max(X_sample[:, i])]\n",
    "            ylim = [np.min(X_sample[:, j]),np.max(X_sample[:, j])]\n",
    "\n",
    "            ax = fig.add_subplot(n_rows, n_cols, plot_counter)\n",
    "            im = ax.scatter(X_sample[:, i], X_sample[:, j], c=y_sample, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "            ax.set_xlim(xlim)\n",
    "            ax.set_ylim(ylim)\n",
    "            ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "            ax.yaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "            ax.set_xlabel('diagnostic' + str(i), fontsize=14)\n",
    "            ax.set_ylabel('diagnostic' + str(j), fontsize=14)\n",
    "\n",
    "            # legend:\n",
    "            ax.text(0.1,0.20, \"starburst\", color='blue', transform=ax.transAxes, fontsize=14)\n",
    "            ax.text(0.1,0.10, \"shocked\",   color='red',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. EXERCISE: Classify starburst/shocked objects\n",
    "\n",
    "### TASK A.1: Choose 3 diagnostics and perform classiffication using linear SVM in 3D\n",
    "\n",
    "### TASK A.2: Choose 2 diagnostics and perform classiffication using non-linear SVM (e.g. with Gaussian or polynomial kernel) in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO A.1: Choosing 3 diagnostics and performing classiffication using linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING DIAGNOSTIC\n",
    "\n",
    "# Choose your diagnostics, which will be now renamed \"A\", \"B\", \"C\" in the following plots\n",
    "\n",
    "X = np.stack((diagnostic_ ..., # diagnostic_A\n",
    "              diagnostic_ ..., # diagnostic_B\n",
    "              diagnostic_ ...  # diagnostic_C\n",
    "             ),axis=-1)\n",
    "\n",
    "y = y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZING DIAGNOSTICS OF INTEREST\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   blue <-> starburst\n",
    "#   red  <-> shocked\n",
    "cmap = mpl.colors.ListedColormap(['blue','red'])\n",
    "\n",
    "# Remember that the sample X is organized as:\n",
    "#  X[:,0] <-> diagnostic A\n",
    "#  X[:,1] <-> diagnostic B\n",
    "#  X[:,2] <-> diagnostic C\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ylim = [np.min(X[:, 0]),np.max(X[:, 0])]\n",
    "\n",
    "# > left plot\n",
    "\n",
    "xlim = [np.min(X[:, 2]),np.max(X[:, 2])]\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "im = ax.scatter(X[:, 2], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_xlabel('diagnostic_C', fontsize=14)\n",
    "ax.set_ylabel('diagnostic_A', fontsize=14)\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.15, \"starburst\", color='blue', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"shocked\",   color='red',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > central plot\n",
    "\n",
    "xlim = [np.min(X[:, 1]),np.max(X[:, 1])]\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "im = ax.scatter(X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_xlabel('diagnostic_B', fontsize=14)\n",
    "ax.set_ylabel('diagnostic_A', fontsize=14)\n",
    "\n",
    "# > right plot\n",
    "ax = fig.add_subplot(133, projection='3d')\n",
    "ax.view_init(10, -110)  # set camera position for better visualization\n",
    "ax.scatter(X[:, 2], X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlabel('diagnostic C', fontsize=14)\n",
    "ax.set_ylabel('diagnostic B', fontsize=14)\n",
    "ax.set_zlabel('diagnostic A', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample in a training [75% of total] and test [25% of total] subsets:\n",
    "(X_train, X_test), (y_train, y_test) = ...\n",
    "\n",
    "N_tot = len(y)          # number of stars\n",
    "N_train = len(y_train)  # size of training sample\n",
    "N_test = len(y_test)    # size of test sample\n",
    "N_diagnostics = np.arange(1, X.shape[1] + 1)    # number of available diagnostics\n",
    "\n",
    "print(\"Sample size\")\n",
    "print(\"----------------\")\n",
    "print(\"Total           | %d\" % (len(X)))\n",
    "print(\" '-> Train      | %d\" % (len(X_train)))\n",
    "print(\"     data shape |\", ((X_train.shape)))\n",
    "print(\" '-> Test       | %d\" % (len(X_test)))\n",
    "print(\"     data shape |\", ((X_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING LINEAR SVM CLASSIFICATION\n",
    "\n",
    "clf = SVC(...)\n",
    "clf. ...\n",
    "y_test_pred = clf. ...\n",
    "\n",
    "completeness, contamination = ...\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"--------\")\n",
    "\n",
    "print(\"completeness: %.4f \" % (completeness), end=\"\")\n",
    "print(\"contamination: %.4f\" % (contamination))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the 3D boundary\n",
    "\n",
    "If you did things right, the next block should help you visualizing the SVM decision hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE DECISION BOUNDARY\n",
    "\n",
    "#%matplotlib notebook\n",
    "# Will allow to inspect the image:\n",
    "# - left click+drag  to rotate\n",
    "# - right click+drag to zoom\n",
    "#\n",
    "# For a static image, uncomment the following line:\n",
    "%matplotlib inline\n",
    "\n",
    "# The equation of the separating plane is given by all x in R^3 such that:\n",
    "# np.dot(svc.coef_[0], x) + b = 0. We should solve for the last coordinate\n",
    "# to plot the plane in terms of x and y.\n",
    "#\n",
    "# See: https://stackoverflow.com/questions/36232334/plotting-3d-decision-boundary-from-linear-svm\n",
    "\n",
    "z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x-clf.coef_[0][1]*y) / clf.coef_[0][2]\n",
    "\n",
    "xlim = [np.min(X_train[:, 2]),np.max(X_train[:, 2])]\n",
    "ylim = [np.min(X_train[:, 1]),np.max(X_train[:, 1])]\n",
    "\n",
    "XX, YY = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# > left plot\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.view_init(10, -110)  # set camera position for better visualization\n",
    "ax.scatter(X_train[:, 2], X_train[:, 1], X_train[:, 0], c=y_train, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlabel('diagnostic C', fontsize=14)\n",
    "ax.set_ylabel('diagnostic B', fontsize=14)\n",
    "ax.set_zlabel('diagnostic A', fontsize=14)\n",
    "\n",
    "# > right plot\n",
    "ax  = fig.add_subplot(122, projection='3d')\n",
    "ax.view_init(10, -110)  # set camera position for better visualization\n",
    "ax.plot_surface(XX, YY, z(XX,YY))\n",
    "ax.scatter(X_train[:, 2], X_train[:, 1], X_train[:, 0], c=y_train, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlabel('diagnostic C', fontsize=14)\n",
    "ax.set_ylabel('diagnostic B', fontsize=14)\n",
    "ax.set_zlabel('diagnostic A', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Which combination of diagnostics yielded best results?\n",
    "\n",
    "Compare completeness and contamination obtained with different choices to evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO A.2: Choosing 2 diagnostics and performing classiffication using SVM with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING DIAGNOSTIC\n",
    "\n",
    "# Choose your diagnostics, which will be now renamed \"A\" and \"B\" in the following plots\n",
    "\n",
    "X = np.stack((diagnostic_ ..., # diagnostic_A\n",
    "              diagnostic_ ...  # diagnostic_B\n",
    "             ),axis=-1)\n",
    "\n",
    "y = y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZING DIAGNOSTICS OF INTEREST\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "# Creating a colormap where:\n",
    "#   blue <-> starburst\n",
    "#   red  <-> shocked\n",
    "cmap = mpl.colors.ListedColormap(['blue','red'])\n",
    "\n",
    "# Remember that the sample X is organized as:\n",
    "#  X[:,0] <-> diagnostic A\n",
    "#  X[:,1] <-> diagnostic B\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.4)\n",
    "\n",
    "ylim = [np.min(X[:, 0]),np.max(X[:, 0])]\n",
    "\n",
    "# plot\n",
    "\n",
    "xlim = [np.min(X[:, 1]),np.max(X[:, 1])]\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.scatter(X[:, 1], X[:, 0], c=y, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_xlabel('diagnostic_B', fontsize=14)\n",
    "ax.set_ylabel('diagnostic_A', fontsize=14)\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.15, \"starburst\", color='blue', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"shocked\",   color='red',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample in a training [75% of total] and test [25% of total] subsets:\n",
    "(X_train, X_test), (y_train, y_test) = ...\n",
    "\n",
    "N_tot = len(y)          # number of stars\n",
    "N_train = len(y_train)  # size of training sample\n",
    "N_test = len(y_test)    # size of test sample\n",
    "N_diagnostics = np.arange(1, X.shape[1] + 1)    # number of available diagnostics\n",
    "\n",
    "print(\"Sample size\")\n",
    "print(\"----------------\")\n",
    "print(\"Total           | %d\" % (len(X)))\n",
    "print(\" '-> Train      | %d\" % (len(X_train)))\n",
    "print(\"     data shape |\", ((X_train.shape)))\n",
    "print(\" '-> Test       | %d\" % (len(X_test)))\n",
    "print(\"     data shape |\", ((X_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING SVM CLASSIFICATION WITH KERNEL\n",
    "\n",
    "clf = SVC(...)   # gaussian\n",
    "#clf = SVC(...)   # polynomial\n",
    "\n",
    "clf. ...\n",
    "y_test_pred = clf. ...\n",
    "\n",
    "completeness, contamination = ...\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"--------\")\n",
    "\n",
    "print(\"completeness: %.4f \" % (completeness), end=\"\")\n",
    "print(\"contamination: %.4f\" % (contamination))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the 2D boundary\n",
    "\n",
    "If you did things right, the next block should help you visualizing the SVM decision hypercurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE DECISION BOUNDARY\n",
    "%matplotlib inline\n",
    "\n",
    "xlim = [np.min(X_train[:, 1]),np.max(X_train[:, 1])]\n",
    "ylim = [np.min(X_train[:, 0]),np.max(X_train[:, 0])]\n",
    "\n",
    "# Creating a grid Z of predictions:\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "# Smooth the boundary:\n",
    "#from scipy.ndimage import gaussian_filter\n",
    "#Z = gaussian_filter(Z, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "# plot\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('diagnostic_B', fontsize=14)\n",
    "ax.set_ylabel('diagnostic_A', fontsize=14)\n",
    "ax.contourf(xx, yy, Z > 0, alpha=0.1, cmap=cmap)\n",
    "ax.contour(xx, yy, Z, colors=['k'], linestyles=['-'],levels=[0])\n",
    "ax.scatter(X_train[:, 1], X_train[:, 0], c=y_train, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.15, \"starburst\", color='blue', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"shocked\",   color='red',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Which method (3D linear vs. 2D non-linear) yielded best results? Which is best to use?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "In the specific case presented above, the two classifiers performs very similarly.\n",
    "Under this aspect, the two methods are formally equivalent.\n",
    "\n",
    "However consider that, while 3D approach seems in general preferrable because it uses the most information (i.e. diagnostics), the 2D model we produced might be more easily used by other researches who only have 2 available diagnostics for their samples.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. EXERCISE: Calibrate classifier hyperparameter\n",
    "\n",
    "### TASK B.1: Run the KNN classifier on the starburst/shocked sample for different values of $k$\n",
    "\n",
    "### TASK B.2: Choose the $k$ which maximizes accuracy without overfitting and asses its performance\n",
    "\n",
    "To achieve this, you will first need to split the sample in a training, validation, and test set.\n",
    "\n",
    "Then, for each $k$ you will have to:\n",
    "- train the classifier (i.e. fit the model) on the training set\n",
    "- evaluate the prediction (i.e. completeness and contamination) on the validation set\n",
    "\n",
    "After having identified the $k$ wich best compromises completenss and contamination, assess the model performance (i.e. calculate once more completenss and contamination) on the independent test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading and setting up the data\n",
    "\n",
    "# CREATING DATA STRUCTURE\n",
    "\n",
    "# > Loading the emission line data and classifications:\n",
    "\n",
    "PATH_lines_data = \"data/emission_lines.txt\" \n",
    "\n",
    "data = np.genfromtxt(PATH_lines_data)\n",
    "# The data file is organized in 8080 lines (i.e. different objects), and 10 columns\n",
    "\n",
    "diagnostic_0 = data[:,3]\n",
    "diagnostic_1 = data[:,4]\n",
    "diagnostic_2 = data[:,5]\n",
    "diagnostic_3 = data[:,6]\n",
    "diagnostic_4 = data[:,7]\n",
    "diagnostic_5 = data[:,8]\n",
    "diagnostic_6 = data[:,9]\n",
    "\n",
    "labels = np.genfromtxt(PATH_lines_data, delimiter=' ', usecols=0, dtype=str)\n",
    "# reading labels from first column\n",
    "# Emission class labelling scheme:\n",
    "#   0 <-> starburst\n",
    "#   1 <-> shocked\n",
    "\n",
    "# Dictionary containg class name and associated label:\n",
    "from collections import OrderedDict\n",
    "classes = OrderedDict()\n",
    "classes[\"starburst\"] = 0\n",
    "classes[\"shocked\"]   = 1\n",
    "\n",
    "labels = [int(float(label)) for label in labels]\n",
    "# converting labels from strings to integers\n",
    "\n",
    "# > Organizing data in an analysis-ready fashion:\n",
    "# NOTE: We will only use 2 diagnostics for this exercise\n",
    "X_sample = np.stack((diagnostic_0,\n",
    "              diagnostic_4),axis=-1)\n",
    "y_sample = labels\n",
    "\n",
    "print('Sample shape:')\n",
    "print(\"_____________________________________\")\n",
    "print('  X  | ' + str(X_sample.shape))\n",
    "print('     | ' + str(X_sample.shape[0]) + ' sammples x ' + str(X_sample.shape[1]) + ' diagnostics' )\n",
    "print(\"-----|-------------------------------\")\n",
    "print('  y  | ' + str(len(y_sample)) + ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample in a training [75% of total] and test [25% of total] subsets:\n",
    "(X_train, X_valid, X_test), (y_train, y_valid, y_test) = ...\n",
    "\n",
    "print(\"Sample size\")\n",
    "print(\"----------------\")\n",
    "print(\"Total           | %d\" % (len(X)))\n",
    "print(\" '-> Train      | %d\" % (len(X_train)))\n",
    "print(\"     data shape |\", ((X_train.shape)))\n",
    "print(\" '-> Validation | %d\" % (len(X_valid)))\n",
    "print(\"     data shape |\", ((X_test.shape)))\n",
    "print(\" '-> Test       | %d\" % (len(X_test)))\n",
    "print(\"     data shape |\", ((X_test.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO B.1: Running the KNN classifier on the starburst/shocked sample for different values of $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLASSIFICATION FOR VARIOUS VALUES OF k\n",
    "\n",
    "# for each 'k', store the classifier and predictions on test sample\n",
    "classifiers = [] \n",
    "predictions = []\n",
    "kvals = ... # k values to be used\n",
    "kvals = list(kvals)\n",
    "\n",
    "for k in kvals:\n",
    "# trying different k hyperparameters\n",
    "\n",
    "    clf = ... # define the classifier, in this case KNN with hyperparameter k\n",
    "    clf. ...     # fit training data\n",
    "    y_valid_pred = ... # predict class of validation set\n",
    "\n",
    "    classifiers.append(clf)\n",
    "    predictions.append(y_valid_pred)\n",
    "\n",
    "# Use dedicated astroML to obtain completeness and contamination:\n",
    "completeness, contamination = ...\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"--------\")\n",
    "\n",
    "for i,k in enumerate(kvals):\n",
    "    print(\"k = %-2s | \" % k, end='')\n",
    "    print(\"completeness: %.4f \" % (completeness[i]), end=\"\")\n",
    "    print(\"contamination: %.4f\" % (contamination[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND PLOT DECISION BOUNDARY FOR 1 CASE, PLOT MODEL PERFORMANCE\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "k_idx = ...\n",
    "# your best choice of k\n",
    "\n",
    "clf = classifiers[k_idx]\n",
    "# classifier for one selected k value\n",
    "\n",
    "xlim = [np.min(X_train[:, 1]),np.max(X_train[:, 1])]\n",
    "ylim = [np.min(X_train[:, 0]),np.max(X_train[:, 0])]\n",
    "\n",
    "# Creating a grid Z of predictions:\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "# Smooth the boundary:\n",
    "#from scipy.ndimage import gaussian_filter\n",
    "#Z = gaussian_filter(Z, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# > left plot: data and decision boundary\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_title('$k$ = %s' % kvals[k_idx] , fontsize=14)\n",
    "ax.set_xlabel('diagnostic B', fontsize=14)\n",
    "ax.set_ylabel('diagnostic A', fontsize=14)\n",
    "ax.contourf(xx, yy, Z > 0, alpha=0.1, cmap=cmap)\n",
    "ax.contour(xx, yy, Z, colors=['k'], linestyles=['-'],levels=[0])\n",
    "ax.scatter(X_train[:, 1], X_train[:, 0], c=y_train, s=2, lw=0, cmap=cmap, zorder=2)\n",
    "\n",
    "# legend:\n",
    "ax.text(0.1,0.15, \"starburst\", color='blue', transform=ax.transAxes, fontsize=14)\n",
    "ax.text(0.1,0.10, \"shocked\",   color='red',  transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > top-right plot: completeness vs k\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(kvals, completeness, 'o--k' , ms=6)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.plot(kvals[k_idx], completeness[k_idx], 'o--k' , color='green', ms=20, mfc='none')\n",
    "ax.set_title('Classifier performance', fontsize=14)\n",
    "ax.set_ylabel('completeness', fontsize=14)\n",
    "ax.set_xlim(np.min(kvals)-1, np.max(kvals)+1)\n",
    "ax.set_ylim(0.97, 1.0)\n",
    "ax.grid(True)\n",
    "\n",
    "# > bottom-right plot: contamination vs k\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(kvals, contamination, 'o--k' , ms=6)\n",
    "ax.plot(kvals[k_idx], contamination[k_idx], 'o--k' , color='green', ms=20, mfc='none')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(2))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('$k$', fontsize=14)\n",
    "ax.set_ylabel('contamination', fontsize=14)\n",
    "ax.set_xlim(np.min(kvals)-1, np.max(kvals)+1)\n",
    "ax.set_ylim(0.0, 0.04)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESPONSE TO B.2: Choosing the $k$ which maximizes accuracy without overfitting and assessing its performance\n",
    "\n",
    "What we need is a hyperparameter $k$ which corresponds to the best tradeoof between the largest completeness and minimal contamination.\n",
    "The plot above suggests that $k$ = 12 arguably provides best compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLASSIFIER ASSESSMENT USING THE TEST SET\n",
    "\n",
    "k_idx = ...\n",
    "# your best choice of k\n",
    "\n",
    "clf = classifiers[k_idx]\n",
    "# classifier for one selected k value\n",
    "\n",
    "y_test_pred = ... # predict class of test set\n",
    "\n",
    "# Use dedicated astroML to obtain completeness and contamination:\n",
    "completeness_test, contamination_test = ...\n",
    "\n",
    "print(\"Test summary:\")\n",
    "print(\"-------------\")\n",
    "\n",
    "print(\"completeness: %.4f \" % (completeness_test), end=\"\")\n",
    "print(\"contamination: %.4f\" % (contamination_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What are the roles of the training, validation and test samples in the context of classification?\n",
    "\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "The **training** sample is used to train the model, i.e. to fit the model parameters. In the SVM classifier, the model parameters would be the matrix $w$ and bias $b$.\n",
    "\n",
    "A **validation** sample, independent from the test sample, is used to calibrate hyperparameters, i.e. the parameters not fitted by the model. In the SVM classifier with a polynomial kernel, the hyperparameter is degree of the polynomial. In the KNN classifier, the hyperparameter is $k$. The idea is to check that the model does not perform well only on the specific set on which it has been trained.\n",
    "\n",
    "The **test** sample is necessary to asses the performance of the classifier model based on the choice of the optimail hyperparameters.\n",
    "\n",
    "That is why, when reporting about a classifier in a paper, it is funamental to present both model _and_ its training set.\n",
    "\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
