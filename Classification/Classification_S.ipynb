{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification - Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification as a prototype of machine learning technique\n",
    "\n",
    "From this session and on, we will enter deeply into the realm of **machine learning (ML)**. We will see that a common pattern will be the necessity of training a model, the assessment of the model preformance, and finally the application of the model to the data to be studied.\n",
    "\n",
    "One typical example of machine learning is the classification problem. This is the process of attributing a \"class\" (or equivalently, a \"label\") to an object of an arbitrary type (e.g. a string or an image) in order to catalogue it based on its properties (the actual information that we pass to the machine, e.g. the pixel intensities in the case of an image).\n",
    "\n",
    "<table><tr>\n",
    "    <td width=260>\n",
    "        <img src=\"images/classification.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.1. Schematic classification in a 2D plot.\n",
    "            <br>\n",
    "            (From [here](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "### 1.2 Types of Classifications\n",
    "\n",
    "Classification can come in 2 major flavors, based on the type of intervention by the user:\n",
    "\n",
    "- **Unsupervised** | The classification is defined \"unsupervised\" when the user does not provide labels during the training process. In other words, the machine does not know a priori what a class is or what properties it should represent. On the contrary, it has to independently learn the definition of each \"class\" using the training data and only _then_ proceed to classify the actual data. For example we would like to distinguish two overlapping \"clouds\" in a scatter plot, and then classify future measurements based on which cloud they fall on. The user can assist the machine in splitting the training data into different classes by imposing the total number of classes, and by running the analysis using the properties which maximise the differences between objects. A classical application of unsupervised classification is represented by \"clustering\" analysis (see relevant session).\n",
    "\n",
    "    _In practice, the machine learns to find similarities between objects with similar properties._\n",
    "\n",
    "\n",
    "- **Supervised** | The classification is defined \"supervised\" when the user provides a label for each object in the training set. In this case, the idea is that we can train the model to associate the label with some given characteristics of the training data.\n",
    "\n",
    "    _In practice, the machine learns to find similarities between objects with the same label._\n",
    "\n",
    "In the reminder, we will only focus on the latter type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Supervised classification - generative / discriminative classification\n",
    "\n",
    "One approach to the classification problem involves the recognition of the function describing the density distribution of each class in the parameter space. This type of problem is called **generative classification** because it implies that we can find the distribution from which the data are generated (or better said, sampled).\n",
    "\n",
    "**Discriminative classification** instead, includes any method which separates classes by \"drawing\" a boundary in the parameter space.\n",
    "\n",
    "In Bayesian terms, generative classifiers model the joint probability (the likelihood **P(D|θ)**) while discriminative classifiers model the conditional probability (the posterior **P(θ|D)**) starting from the prior (**P(θ)**).\n",
    "\n",
    "There exists a continously growing number of discriminative classifiers, therefore we will only concentrate on a representative selection to illustrate the generic approach. Namely, we will use:\n",
    "- **$k$-nearest neighbor (KNN)**\n",
    "- **Support Vector Machine (SVM)**\n",
    "\n",
    "\n",
    "### 1.4. Model evaluation metrics\n",
    "\n",
    "One more concept before introducing a real example: **how do we assess the performance of a classification?**\n",
    "\n",
    "There exist a number of assessment metrics based on **confusion matrixes** like (but not limited to) the one in Figure 1.2.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=600>\n",
    "        <img src=\"images/confusion_matrix.png\">\n",
    "        <center>\n",
    "            <br>\n",
    "            Figure 1.2. **Confusion matrix** for classification.\n",
    "            <br>\n",
    "            (From [here](https://towardsdatascience.com/precision-vs-recall-386cf9f89488))\n",
    "        </center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "In astrophysics we usually concentrate on **completeness** and **contamination** (see [this lik](http://connolly.github.io/introAstroML/blog/classification.html)):\n",
    "\n",
    "$$completeness = \\frac{True~Positives}{All~real~Positives} = \\frac{True~Positives}{True~Positives + False~Negatives} = recall$$\n",
    "\n",
    "$$contamination = \\frac{False~Positives}{All~detected~Positives} = \\frac{True~Positives}{True~Positives + False~Positives}$$\n",
    "\n",
    "We will explore these metrics throught the next sessions.\n",
    "<br>\n",
    "Additionally, we will see how the assessment requires us to split the sample into a:\n",
    "- **training** set\n",
    "- **validation** set\n",
    "- **test** set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The sample for our classification example: RR Lyrae variables\n",
    "\n",
    "RR Lyrae are bright ($M_V \\approx 0.6~{\\rm mag}$), helium-burning, pulsating (see helium partial ionization zones) stars with period less than a day $\\left(6{\\rm h} \\lesssim P \\lesssim 18{\\rm h}\\right)$ and visual amplitudes in the range $\\left(0.3, 1.2\\right){\\rm mag}$.\n",
    "\n",
    "<img src=\"images/CMD_RR_Lyrae.svg\" width=300>\n",
    "<center>Figure 2.1. Location of the RR Lyrae in the CMD.</center>\n",
    "\n",
    "They are found in the galaxy, predominately in old population regions, like the galactic center, globular clusters and halo, as well as in its satellites (e.g. Magellanic Clouds).\n",
    "\n",
    "### Astronomical relevance\n",
    "* Being old population variables, they are used as **test subjects** for stellar structure and evolution models\n",
    "* As they are found in the halo and on the disk, they are **relics** of the star-formation history, chemical evolution and kinematics of the galaxy\n",
    "* Having approximately the same luminosity, they qualify as distance indicators for $d \\lesssim 100~{\\rm kpc}$\n",
    "\n",
    "For more info consult: [Bono (2003), LNP 635, 85B](http://esoads.eso.org/abs/2003LNP...635...85B) and [this link](https://www.eso.org/sci/publications/messenger/archive/no.13-jun78/messenger-no13-15-17.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading the necessary packages and data\n",
    "\n",
    "The data for the example will be retrieved via the `astroML` package (see `astroML.datasets`). From this, we will fetch colors for a sample of SDSS (Sloan Digital Sky Survey) stars including both RR Lyrae and any other type of stars. The `fetch_rrlyrae_combined` function (below) downloads two arrays:\n",
    "* a $N \\times 4$ array for the four colors $g-r$, $u-g$, $r-i$, $i-z$ of $N \\approx 100K$ stars\n",
    "* a $N$-sized array of zeros and ones, where \"$1$\" is the label which denotes that the respective star is an RR Lyrae\n",
    "\n",
    "### A technical workaround\n",
    "\n",
    "As the data to download is about $70$ MB (and may take a while with the public wifi network) and is permenently stored in your computer, we downloaded the data and stored it in a compressed `numpy` format. The function that performs exactly that, is `download_and_save_RRlyrae_data()` and we include it only for future reference.\n",
    "\n",
    "Now, the script only requires calling `load_data()` and having the file `RRlyrae.npz` in the working directory. In case the latter is missing, just uncomment the call to the function `download_and_save_RRlyrae_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from astroML.datasets import fetch_rrlyrae_combined\n",
    "from astroML.utils import split_samples\n",
    "from astroML.utils import completeness_contamination\n",
    "\n",
    "\n",
    "PATH = \"data/RRlyrae.npz\"\n",
    "\n",
    "\n",
    "def download_and_save_RRlyrae_data():\n",
    "    \"\"\"Create the local file containing photometry and classification of stars in SDSS sample.\"\"\"\n",
    "    \n",
    "    # get data and split into training & testing sets\n",
    "    print(\"Getting RR Lyrae data...\")\n",
    "    X, y = fetch_rrlyrae_combined()\n",
    "    X = X[:, [1, 0, 2, 3]]           # rearrange columns for better plots\n",
    "    np.savez_compressed(PATH, colors=X, isRR=y)\n",
    "    print(\"    Done.\")\n",
    "    \n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"Load RR Lyrae data from local file.\"\"\"\n",
    "    \n",
    "    print(\"Loading data from {}...\".format(PATH))\n",
    "    data = np.load(PATH)\n",
    "    colors, isRR = data[\"colors\"], data[\"isRR\"]\n",
    "    print(\"    Done.\")\n",
    "    print(\"    {:6} objects in total\".format(len(isRR)))\n",
    "    print(\"    {:6} RR Lyrae in data\".format(int(sum(isRR))))\n",
    "    return colors, isRR\n",
    "\n",
    "\n",
    "# if the file is missing, uncomment the following line\n",
    "# download_and_save_RRlyrae_data()\n",
    "\n",
    "# load photometry and classification (labels) of training data\n",
    "X, y = load_data()\n",
    "# X = <Nx4 colors>\n",
    "# y = <N> labels\n",
    "\n",
    "# Create boolean \"mask\" arrays denoting classification as RR Lyrae:\n",
    "# (will be useful from data exploration/plotting)\n",
    "isRR = (y == 1)\n",
    "noRR = (y == 0)\n",
    "\n",
    "# Limit scatter plots (not histograms) in showing a maximum of 5000 non-RR Lyrae stars:\n",
    "N_plot = 5000 + int(sum(y))\n",
    "noRR[:-N_plot] = False\n",
    "\n",
    "# names of the colors\n",
    "color_names = [\"$g-r$\", \"$u-g$\", \"$r-i$\", \"$i-z$\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring the data\n",
    "\n",
    "### Color distributions\n",
    "\n",
    "We will start easy by creating color histograms for \"standard\" and RR Lyrae stars to see if they are separable by using just one colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    color = X[:, i]\n",
    "    bins = np.linspace(np.nanmin(color), np.nanmax(color), 31)\n",
    "    \n",
    "    plt.subplot(221 + i)\n",
    "    plt.hist(color[isRR],  bins=bins, log=True, color=\"r\", histtype=\"step\", label=\"RR lyrae\")\n",
    "    plt.hist(color[~isRR], bins=bins, log=True, color=\"k\", histtype=\"step\", label=\"stars\")\n",
    "    plt.xlabel(color_names[i]  , fontsize=14)\n",
    "    plt.legend(loc=\"upper left\", fontsize=14)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Why do we use colors rather than magnitudes?\n",
    "\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "* Colors are independent from distance\n",
    "* Colors give direct information on the shape of spectrum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer 2D look\n",
    "Maybe using color-color diagrams we can easily classify RR Lyrae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "k = 1\n",
    "for i in range(4):\n",
    "    c1 = X[:, i]\n",
    "    for j in range(i + 1, 4):\n",
    "        c2 = X[:, j]\n",
    "        plt.subplot(320 + k)\n",
    "        plt.plot(c1[noRR], c2[noRR], \"k.\", label=\"stars\")\n",
    "        plt.plot(c1[isRR], c2[isRR], \"r.\", label=\"RR lyrae\")\n",
    "        plt.xlabel(color_names[i], fontsize=14)\n",
    "        plt.ylabel(color_names[j], fontsize=14)\n",
    "        plt.legend(loc=\"upper right\", framealpha=0.9, mode=\"expand\", ncol=2, fontsize=14)\n",
    "        k += 1\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As close as we can *see* (3D)\n",
    "What if we plot three colors? Of course we cannot continue with four colors because of our 3D perception limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "combinations = [(0, 1, 2), (0, 1, 3), (0, 2, 3), (1, 2, 3)]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "for index, combination in enumerate(combinations):\n",
    "    i, j, k = combination\n",
    "    ax = fig.add_subplot(221 + index, projection='3d')\n",
    "    ax.view_init(60, -130)  # set camera position for better visualization\n",
    "    ax.scatter(X[:, i][noRR], X[:, j][noRR], X[:, k][noRR], c=[0.5,0.7,0.7], marker=\"o\", alpha=0.5, edgecolors=\"k\", label=\"stars\")\n",
    "    ax.scatter(X[:, i][isRR], X[:, j][isRR], X[:, k][isRR], c=\"r\"                                 , edgecolors=\"k\", label=\"RR Lyrae\")\n",
    "    ax.set_xlabel(color_names[i], fontsize=14)\n",
    "    ax.set_ylabel(color_names[j], fontsize=14)\n",
    "    ax.set_zlabel(color_names[k], fontsize=14)\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D plots do not significantly improve the *manual classification*, compared to the 2D plots. In both cases, we see a *tail* where RR Lyrae variables reside.\n",
    "\n",
    "### Q: What we would be compromizing if we were to separate RR Lyrae by applying simple horizontal/vertical \"cuts\" in the 1D, 2D, or 3D color distributions?\n",
    "\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"Completenss versus contamination\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The need for a ML classifier\n",
    "We can improve over the limitations of the *manual classification* by obtaining:\n",
    "    \n",
    "* mathematical formulation\n",
    "* automation\n",
    "* estimates on completeness and contamination of the classification method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $k$-Nearest Neighbors (KNN) classification\n",
    "\n",
    "Looking at the scatter plots, we find two blobs corresponding to the different classes. Our mind uses the proximity of the points to form a \"mental boundary region\" (in a _discriminative classification_ fashion). In this specific case we can try to automate of the procedure, as well as the extension to arbitrary number of dimensions, by mimicking the human behaviour, i.e. by:\n",
    "\n",
    "> attributing a given point to the class that dominates its surroundings.\n",
    "\n",
    "The problem is then how to define the \"neighborhood\" of a point. The trivial solution would be to set a fixed radius. The issue then becomes its size: if too small, we **will not find neighbors** for \"satellite\" points at the edge of a class cluster; if too large, we **will lose resolution** in dense parts, effectively throwing away information. Therefore, ideally we would like to have a *variable bandwidth* selection threshold.\n",
    "\n",
    "> One solution is to use a local average of the labels of the $k$ nearest neighbors:\n",
    ">\n",
    "> $y = \\frac{1}{k} \\sum￼_{x_i \\in N_k(x)} y_{i}$\n",
    ">\n",
    "> where N_k(x) is the neighborhood around $x_i$\n",
    "\n",
    "In this way the classification _is not_ defined based on the distance on the parameter graph, but is rather scale-independent.\n",
    "\n",
    "Let's see a 2D example. We got two parameters and training data that are classified as being *red* or *blue*. The question is how do we classify a new (_i.e. not part of the training set_) point? The following images are taken from [here](https://importq.wordpress.com/2017/11/24/mnist-analysis-using-knn/) (we edited the the first one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated example\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_initial.jpg\">\n",
    "        <center>Figure 3.1.a. Training data already possessing a _red_ or _blue_ label, and an arbitrary new point to be classified.</center>\n",
    "    </td>    \n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh.gif\">\n",
    "        <center>Figure 3.1.b. Classification using majority votes of $k$ neighbors, fordifferent values of $k$.</center>\n",
    "    </td>\n",
    "    <td width=400>\n",
    "        <img src=\"images/knn_neigh_mult.gif\">\n",
    "        <center>Figure 3.1.c. For a fixed $k$, the **model** can be thought as of a function of the location in the parameter space. Note that the appearing dots are _not_ part of the training set. Instead, they represent the predicted classifications **if** the new point would fall on that position.</center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The panels *a* and *b* already suggest that the KNN classification will be affected by the choice of the **hyperparameter** $k$: we will address this issue later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Applying $k$-NN on the RR Lyrae photometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sample in a training [75% of total] and test [25% of total] subsets:\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.75, 0.25], random_state=0)\n",
    "\n",
    "N_tot = len(y)          # number of stars\n",
    "N_st = np.sum(y == 0)   # number of non-RR Lyrae stars\n",
    "N_rr = N_tot - N_st     # number of RR Lyrae\n",
    "N_train = len(y_train)  # size of training sample\n",
    "N_test = len(y_test)    # size of test sample\n",
    "N_plot = 5000 + N_rr    # number of stars plotted (for better visualization)\n",
    "Ncolors = np.arange(1, X.shape[1] + 1)    # number of available colors\n",
    "\n",
    "print(\"Sample size\")\n",
    "print(\"----------------\")\n",
    "print(\"Total           | %d\" % (len(X)))\n",
    "print(\" '-> Train      | %d\" % (len(X_train)))\n",
    "print(\"     data shape |\", ((X_train.shape)))\n",
    "print(\" '-> Test       | %d\" % (len(X_test)))\n",
    "print(\"     data shape |\", ((X_test.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM CLASSIFICATION FOR VARIOUS VALUES OF k\n",
    "\n",
    "# for each 'k', store the classifier and predictions on test sample\n",
    "classifiers = []\n",
    "predictions = []\n",
    "kvals = [1, 3, 10] # k values to be used\n",
    "\n",
    "for k in kvals:\n",
    "# trying different k hyperparameters\n",
    "\n",
    "    classifiers.append([])\n",
    "    predictions.append([])\n",
    "    \n",
    "    for nc in Ncolors:\n",
    "    # for each k, we will use between 1 and 4 colors (1D to 4D) to classify the sources\n",
    "    \n",
    "        clf = KNeighborsClassifier(n_neighbors=k) # define the classifier, in this case KNN with hyperparameter k\n",
    "        clf.fit(X_train[:, :nc], y_train)         # fit training set\n",
    "        y_pred = clf.predict(X_test[:, :nc])      # predict class of test set\n",
    "\n",
    "        classifiers[-1].append(clf)\n",
    "        predictions[-1].append(y_pred)\n",
    "\n",
    "# Use dedicated astroML to obtain completeness and contamination:\n",
    "completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"--------\")\n",
    "\n",
    "for i,k in enumerate(kvals):\n",
    "    print(\"k =\", k)\n",
    "    for nc in Ncolors:\n",
    "        print(\"\\tno. of colors = %s | \" % (nc), end=\"\")\n",
    "        print(\"completeness: %.2f \" % (completeness[i,nc-1]), end=\"\")\n",
    "        print(\"contamination: %.2f\" % (contamination[i,nc-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE AND PLOT DECISION BOUNDARY\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# classifiers\n",
    "# <N_k, N_colors>\n",
    "\n",
    "clf = classifiers[1][1]\n",
    "# classifier with k=3 and the first 2 colors (g-r, and u-g)\n",
    "xlim = (-0.15, 0.4) # g-r limits\n",
    "ylim = (0.7, 1.35)  # u-g limits\n",
    "\n",
    "# Creating a grid Z of predictions:\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71), np.linspace(ylim[0], ylim[1], 81))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Creating a colormap where the RR lyrae (y label = \"1\") are plotted in red\n",
    "cmap = mpl.colors.ListedColormap(['black', 'red'])\n",
    "cmap.set_under('0.5')\n",
    "cmap.set_over('0.5')\n",
    "\n",
    "# PLOT THE RESULTS\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "# > left plot: data and decision boundary\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "im = ax.scatter(X[-N_plot:, 0], X[-N_plot:, 1], c=y[-N_plot:], s=10, lw=0, cmap=cmap, zorder=2)\n",
    "im.set_clim(-0.5, 1.5)\n",
    "ax.contour(xx, yy, Z, 1, colors='blue')\n",
    "# NOTE: The contour showing the locus where a datum is classified as \"RR Lyrae\" is simply\n",
    "#       calculated on the grid of predictions\n",
    "#       To visualize the grid of predictions instead of the contour, use:\n",
    "#im = ax.imshow(Z, origin='lower', aspect='auto', cmap=cmap, zorder=1, alpha=0.1, extent=xlim + ylim)\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_title('Decision boundary for classification', fontsize=14)\n",
    "ax.set_xlabel('$g-r$', fontsize=14)\n",
    "ax.set_ylabel('$u-g$', fontsize=14)\n",
    "ax.text(0.02, 0.02, \"k = %i\" % kvals[1], transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "# > top-right plot: completeness vs Ncolors\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.plot(Ncolors, completeness[0], 'o-k' , ms=6, label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, completeness[1], '^--k', ms=6, label='k=%i' % kvals[1])\n",
    "ax.plot(Ncolors, completeness[2], 'v:k' , ms=6, label='k=%i' % kvals[2])\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_title('Classifier performance', fontsize=14)\n",
    "ax.set_ylabel('completeness', fontsize=14)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "# > bottom-right plot: contamination vs Ncolors\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.plot(Ncolors, contamination[0], 'o-k' , label='k=%i' % kvals[0])\n",
    "ax.plot(Ncolors, contamination[1], '^--k', label='k=%i' % kvals[1])\n",
    "ax.plot(Ncolors, contamination[2], 'v:k' , label='k=%i' % kvals[2])\n",
    "ax.legend(loc='lower right', bbox_to_anchor=(1.0, 0.79), fontsize=14)\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(plt.MultipleLocator(0.2))\n",
    "ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "ax.set_xlabel('N colors', fontsize=14)\n",
    "ax.set_ylabel('contamination', fontsize=14)\n",
    "ax.set_xlim(0.5, 4.5)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.grid(True)\n",
    "\n",
    "### Q: When, in astronomical context, we would care for completeness and when about contamination?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What does the coarsness of the decision boundary (blue contour) indicates?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "That indicates a potential over-fitting of the training sample, i.e. that the boudary adapts too much to this specific choice of the training set.\n",
    "Therefore, when applying this trainer classifier to an other set, we might find that it does not perform well. In fact, the test tset above shows a contamination of $\\sim$50 %.\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: When, in astronomical context, we would care for completeness and when about contamination?\n",
    "[+] click to discover\n",
    "\n",
    "[//]: # \"\n",
    "* completeness: catalogue for further cross-correlation\n",
    "* contamination: sample for expensive follow-up surveys\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Choosing the right $k$\n",
    "\n",
    "### Small $k$ \n",
    "> $\\large+$ only nearby points are taken into account\n",
    "\n",
    "> $\\large-$ if too small, noisy decision boundaries (see change of classification in Figure 3.1.b for small $k$'s)\n",
    "\n",
    "### Large $k$\n",
    "> $\\large+$ assuming infinite sample for $k \\rightarrow \\infty$, the *classification error rate* is minimized\n",
    "\n",
    "> $\\large-$ **but** real-life samples are finite, so large $k$ results to loss of resolution (over-smoothing)\n",
    "\n",
    "### Some approaches\n",
    "\n",
    "> take $k = \\sqrt{N}$\n",
    "\n",
    "> use *cross-validation* to select optimal $k$\n",
    "\n",
    "> if $2$ classes, go for an odd $k$ to avoid ties\n",
    "\n",
    "Figure 3.2 (from [here](https://idc9.github.io/stor390/notes/cross_validation/cross_validation.html)) reports an example of over-smoothing.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"images/k1.jpg\"   width=400></td>    \n",
    "        <td><img src=\"images/k5.jpg\"   width=400></td>\n",
    "        <td><img src=\"images/k399.jpg\" width=400></td>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <center>\n",
    "            Figure 3.2. Variation of classification boundaries as $k$ increases to very large values.\n",
    "        </center>    \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 3.3. Defining the neighborhood\n",
    "What does \"near\" in \"nearest neighbors\" mean?\n",
    "Identifying neighbors requires calculating a **distance** between points or, better said, define a **metric**. A metric might be difficult to define or might not be definable at all for the type of features we are inspecting (e.g. imagine classifying galaxies by morphology and color). Consider for example the following issues.\n",
    "\n",
    "* One simple choice would be to adopt the common Euclidean metric. However that would not be performing properly when the parameter spans are different (e.g. one parameter has span $\\left[1, 100\\right]$ while the other has span $\\left[0, 1\\right]$). This might require **feature weighting** (normalization).\n",
    "* What if all or some characteristics are not numeric but categorial? What is the metric in this case (see Hamming distance)?\n",
    "\n",
    "For more information, consult [this document](http://www.cs.haifa.ac.il/~rita/ml_course/lectures/KNN.pdf).\n",
    "\n",
    "### 3.4. Weighted KNN ($w$KNN)\n",
    "We can mitigate the impact of choosing the appropriate $k$ by imposing that each vote on the class is weighted by the distance (or *similarity*) from the point to be classified. In this case, the classification algorithm for point $x$ becomes:\n",
    "\n",
    "> $y = \\frac{1}{k} \\sum￼_{x_i \\in N_k(x)} ~K(d) \\times y_{i}$\n",
    ">\n",
    "> where $d$ is the distance and $K(d)$ is the **kernel** which converts the distance into a weight. One simple example is the inversion kernel $K(d) = \\frac{1}{d}$\n",
    "\n",
    "For an extensive treatment, consult [Hechenbichler (2004)](https://epub.ub.uni-muenchen.de/1769/1/paper_399.pdf).\n",
    "\n",
    "### 3.5. Final remarks on KNN\n",
    "\n",
    "### Pros\n",
    "* No need to assume distribution (_discriminative classifier_)\n",
    "* Simple and intuitive\n",
    "* Robust for large samples\n",
    "\n",
    "### Cons\n",
    "* Hard to select $k$\n",
    "* Computationally expensive: $O(nkd)$, where $n$ is the size of training sets, and $d$ is the dimension of each training set (but can be optimised)\n",
    "* Good accuracy requires large samples unifromly covering the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machine (SVM) is a way of choosing a linear decision boundary between different classes.\n",
    "\n",
    "The classification boundary is provided by the hyperplane maximizing the distance between the hyperplane itself and the closest point from either class. This distance is called *margin*. Points on the margins are called *support vectors*.\n",
    "\n",
    "<table><tr>\n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_1.png\">\n",
    "        <center>Figure 4.1.a. Hyperplane (dashed line) separating two classes (_red_ and _green_).</center>\n",
    "    </td>    \n",
    "    <td width=400>\n",
    "        <img src=\"images/SVM_2.png\">\n",
    "        <center>Figure 4.1.b. The closest points to the hyperplane from each class constitute the \"tip\" of the *support vectors*.</center>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "Figure 4.1.a shows two different classes (e.g. young and old stars) distributing in a scatter plot according to variable $x_1$ (e.g. radius) and $x_2$ (e.g. temperature). Figure 4.1.b explains the origin of the name \"support vectors\": the closest points _support_ the hyperplanes (solid lines) equally distant from the decision hyperplane (dashed line).\n",
    "\n",
    "Infinite possible boundaries can separate the two classes. SVM algorithms find the one that maximizes the distance between the supported hyperplanes.\n",
    "\n",
    "### 4.1. Hyperplanes and decision boundary\n",
    "\n",
    "The supported hyperplanes (solid-lines in Figure 4.1.a can be defined as:\n",
    "\n",
    "> **w**$\\cdot$**x** + b = +1\n",
    ">\n",
    "> **w**$\\cdot$**x** + b = -1\n",
    "\n",
    "where **x** is the coordinate on the (x1, x2) plane, **w** is a 2$\\times$1 matrix and **b** a scalar. It turns out that these hyperplanes are separated by a distance 2 / ||w||. Finding the ideal classification boundary, i.e. the one maximizing the distance, is therefore a problem of minimizing the norm ||w||. This is what SVM algorithms do.\n",
    "\n",
    "For a complete mathematical formulation, consult [this document by Berwick, R.]( http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf).\n",
    "\n",
    "### 4.2. Separatable classes (or not)\n",
    "\n",
    "We cannot always assume that 2 classes are separble without \"contamination\". That is why SVM algorithms include **slack variables**, a tunable parameter which penalizes misclassifications.\n",
    "\n",
    "\n",
    "### 4.3. Multiple classes\n",
    "\n",
    "The SVM method can be applied for multiple classes as well (Figure 4.3).\n",
    "\n",
    "<img src=\"images/svm_many_classes.png\" width=400>\n",
    "<center>\n",
    "    Figure 4.3: SVM applied to 3 different classes.\n",
    "</center>\n",
    "\n",
    "### 4.4. Multiple dimensions\n",
    "\n",
    "If our sample of stars is characterized by three parameters (X, Y, Z), e.g. radius, temperature and mass, then the scatter plot has 3 dimensions (Figure 4.3). The boundary between the classes in the 3-D plot is a plane. Because of the fact that the method can be extrapolated at N-dimensions, the boundary is a *hyperplane*.\n",
    "\n",
    "<img src=\"images/svm_3d.png\" width=400>\n",
    "<center>\n",
    "    Figure 4.4: Support vector machine applied for 3-D features and three classes.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Applying linear SVM to the RR Lyrae photometric data\n",
    "\n",
    "We will use SVM to our RR Lyrae example to separate objects with a linear decision boundary.\n",
    "In $\\S$4.6 and $\\S$4.7 we will see an extension of SVM adopting non-linear boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this block, we define a function that applies SVM on our data.\n",
    "# If the boolean argument is set to `True`, it applies the linear SVM (this section)\n",
    "# otherwise it uses Gaussian Radial Basis function kernel (RBF; Section 4.7).\n",
    "\n",
    "def apply_SVM(linear):\n",
    "    if linear:\n",
    "        kernel_to_use = \"linear\"\n",
    "        gamma_to_use = \"auto\"\n",
    "    else:\n",
    "        kernel_to_use = \"rbf\"\n",
    "        gamma_to_use = 20.0\n",
    "\n",
    "    def compute_SVM(Ncolors):\n",
    "        classifiers = []\n",
    "        predictions = []\n",
    "        \n",
    "        for nc in Ncolors:\n",
    "            print(\"    Computing for\", nc, \"color(s)...\")\n",
    "            # perform support vector classification\n",
    "            clf = SVC(kernel=kernel_to_use, gamma=gamma_to_use, class_weight='balanced')\n",
    "            clf.fit(X_train[:, :nc], y_train)\n",
    "            y_pred = clf.predict(X_test[:, :nc])\n",
    "\n",
    "            classifiers.append(clf)\n",
    "            predictions.append(y_pred)\n",
    "\n",
    "        return classifiers, predictions\n",
    "\n",
    "    print(\"Performing SVM classification...\")\n",
    "\n",
    "    classifiers, predictions = compute_SVM(Ncolors)\n",
    "\n",
    "    completeness, contamination = completeness_contamination(predictions, y_test)\n",
    "\n",
    "    print(\"completeness\",  completeness)\n",
    "    print(\"contamination\", contamination)\n",
    "\n",
    "    # COMPUTE THE DECISION BOUNDARY\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: The sklearn SVM routine does _not_ return the coefficients of the\n",
    "          decision boundary.\n",
    "          For example, for a 2D case where the decision boundary is a line\n",
    "          of the type:\n",
    "    \n",
    "             y = a*x + b\n",
    "    \n",
    "          one would expect to obtain \"a\" and \"b\" (where b == intercept).\n",
    "          Instead, the w matrix is returned (see Section 4.1).\n",
    "          In the 2D case, for a linear model, the conversion is trivial\n",
    "          (see the code below for a linear case).\n",
    "          \n",
    "          For a more generic case of representing a curved boundary in N\n",
    "          dimensions, it is more convenient to generate a grid (\"Z\") of\n",
    "          predictions covering the whole parameter space (essentially a map\n",
    "          of the classes), and then plot the contour around a class\n",
    "          (see the code below for a non-linear case).\n",
    "          \n",
    "          Alternatively, one can use the sklearn built-in function to\n",
    "          draw the decision function:\n",
    "\n",
    "              https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = classifiers[1]\n",
    "    # loading classifier for 2 colors\n",
    "\n",
    "    if linear:\n",
    "        # > Extracting slope and intercept of boundary line:\n",
    "        w = clf.coef_[0]\n",
    "        a = -w[0] / w[1]\n",
    "        yy = np.linspace(-0.1, 0.4)\n",
    "        xx = a * yy - clf.intercept_[0] / w[1]\n",
    "    else:\n",
    "        # > Creating a grid of predictions:\n",
    "        xlim = (0.7, 1.35)\n",
    "        ylim = (-0.15, 0.4)\n",
    "        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 101), np.linspace(ylim[0], ylim[1], 101))\n",
    "        Z = clf.predict(np.c_[yy.ravel(), xx.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        # Smooth the boundary:\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        Z = gaussian_filter(Z, 2)\n",
    "\n",
    "    # PLOT THE RESULTS\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    fig.subplots_adjust(bottom=0.15, top=0.95, hspace=0.0, left=0.1, right=0.95, wspace=0.2)\n",
    "\n",
    "    # left plot: data and decision boundary\n",
    "    ax = fig.add_subplot(121)\n",
    "    im = ax.scatter(X[-N_plot:, 1], X[-N_plot:, 0], c=y[-N_plot:], s=4, lw=0, cmap=plt.cm.binary, zorder=2)\n",
    "    if linear:\n",
    "        ax.plot(xx, yy, '-k')\n",
    "    else:\n",
    "        ax.contour(xx, yy, Z, [0.5], colors='k')\n",
    "\n",
    "    \n",
    "    im.set_clim(-0.5, 1)\n",
    "    ax.set_xlim(0.7, 1.35)\n",
    "    ax.set_ylim(-0.15, 0.4)\n",
    "    ax.set_xlabel('$u-g$', fontsize=14)\n",
    "    ax.set_ylabel('$g-r$', fontsize=14)\n",
    "\n",
    "    # plot completeness vs Ncolors\n",
    "    ax = fig.add_subplot(222)\n",
    "    ax.plot(Ncolors, completeness, 'o-k', ms=6)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax.set_ylabel('completeness', fontsize=14)\n",
    "    ax.set_xlim(0.7, 4.5)\n",
    "    ax.set_ylim(0.9, 1.04)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # plot contamination vs Ncolors\n",
    "    ax = fig.add_subplot(224)\n",
    "    ax.plot(Ncolors, contamination, 'o-k', ms=6)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.05))\n",
    "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%i'))\n",
    "    ax.set_xlabel('N colors',      fontsize=14)\n",
    "    ax.set_ylabel('contamination', fontsize=14)\n",
    "    ax.set_xlim(0.7, 4.5)\n",
    "    ax.set_ylim(0.8, 1.04)\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_SVM(linear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the best contamination! Let's try something more sophisticated ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Non-linear boundaries\n",
    "\n",
    "Sometimes, linear boundaries may not be optimal and a non-linear SVM should be used instead. The left panel of Figure 4.6 shows an 2D scatter plot of two different classes (e.g. red and green stars with different radii and temperatures) which cannot be linearly separated.\n",
    "\n",
    "In order to find non-linear boundaries we can tackle the problem in an higher dimensional space. We use a process called **kernelization**, which consists in using a kernel function to attribute to our data a value in the additional dimension. Then, we draw the decision hyperplane into this higher dimensional space.\n",
    "\n",
    "The central panel of Figure 4.6 shows that once the 2D data are mapped to a 3D space by attributing a $z$ value through a Gaussian-like function, the classes are easily separable by a 3D hyperplane. Projecting back the plane in 2D, we obtain the non-linear boundary (Figure 4.6, rght panel).\n",
    "\n",
    "<img src=\"images/kernel.png\" width=800>\n",
    "<center>\n",
    "    Figure 4.6. When no linear boundaries can be used the SVM method can be applied by using kernel.\n",
    "</center>\n",
    "\n",
    "### Choosing the kernel function\n",
    "\n",
    "Useful kernel functions shall satisfy specific conditions (see [7]), so that in practice only a few are used. In the example of Figure 4.6, the Gaussian Radial Basis Function is used:\n",
    "\n",
    "> $K(x,y) = e^{-\\gamma(x-y)^2}$\n",
    "\n",
    "where $\\gamma$ is a hyperparameter which shall be learned via cross-validation (in our example we use an arbitrary value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Applying kernelized SVM to the RR Lyrae photometric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_SVM(linear=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved our results, but the contamination is still high.\n",
    "We will probably have to accept this limitation because RR Lyrae simply occupy a locus of the colors diagrams which is also populated by other stellar types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Misclassifications\n",
    "\n",
    "Figure 4.3 shows that the blue points class is contaminated by some green \n",
    "points (*misclassified* points). Likewise, the green points class is contaminated by blue points. This contamination makes it difficult to define the boundary between the two classes.\n",
    "\n",
    "Keep in mind that SVM finds the hyperplane that maximizes the margin and indirectly minimizes the misclassifications. However SVM is not designed to minimize the contamination _per se_.\n",
    "\n",
    "\n",
    "### 4.9. Final remarks on SVM\n",
    "\n",
    "### Pros\n",
    "* Good at dealing with high dimensional data\n",
    "* Works well on small data sets\n",
    "\n",
    "### Cons\n",
    "* Picking the right kernel and parameters can be computationally intensive\n",
    "* It suffers from contamination\n",
    "\n",
    "\n",
    "For further information on SVM, consult [this link](http://www.saedsayad.com/support_vector_machine.htm)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
